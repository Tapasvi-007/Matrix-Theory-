\let\negmedspace\undefined
\let\negthickspace\undefined
\documentclass[journal]{IEEEtran}
\usepackage[a4paper, margin=10mm, onecolumn]{geometry}
\usepackage{lmodern} % Ensure lmodern is loaded for pdflatex
\usepackage{tfrupee} % Include tfrupee package
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\setlength{\headheight}{1cm} % Set the height of the header box
\setlength{\headsep}{0mm}     % Set the distance between the header box and the top of the text

\usepackage{gvv-book}
\usepackage{gvv}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts,amsthm}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{txfonts}
\usepackage{listings}
\usepackage{enumitem}
\usepackage{mathtools}
\usepackage{gensymb}
\usepackage{comment}
\usepackage[breaklinks=true]{hyperref}
\usepackage{tkz-euclide} 
\usepackage{listings}
\usepackage{gvv}                                        
\def\inputGnumericTable{}                                 
%\usepackage[latin1]{inputenc}
\usepackage{color}                                            
\usepackage{array}                                            
\usepackage{longtable}                                       
\usepackage{calc}                                             
\usepackage{multirow}                                         
\usepackage{hhline}                                           
\usepackage{ifthen}                                           
\usepackage{lscape}

\begin{document}
\bibliographystyle{IEEEtran}
\vspace{3cm}

\title{Software Assignment-REPORT}
\author{EE24BTECH11045 - N.Tapasvi}
{\let\newpage\relax\maketitle}


\textbf{AIM:}\\
The aim of this report is to explore various methods for computing and analyzing eigenvalues of matrices.\\

\textbf{THEORY:}\\
\textbf{Eigenvalues and Eigenvector:} A set of scalars that describe how a matrix transforms a vector. They are also called characteristic values or characteristic roots. An eigenvalue of a matrix $A$ is the value of $\lambda$ which satisfies the equation\\ \centerline{A$v$=$\lambda v$} for some non-zero vector $v$. Here $A$ is a $n \times n$ matrix,$\lambda$ is eigenvalue and $v$ is eigenvector. The eigenvectors do not change direction but may be scaled after matrix transformation.\\

For a given matrix $A$, eigenvectors are scaled by matrix $A$ in the direction of the eigenvector. The amount by which the eigenvector is scaled depends on $\lambda$.
\begin{itemize}
	\item $\lambda$ >1, the eigenvector is stretched.
	\item $\lambda$ =1, the length of eigenvector not changed.
	\item 0<$\lambda$<1, the eigenvector is shrunk.
	\item $\lambda$ =0, the matrix transforms the eigenvector into a zero vector.
	\item $\lambda$ =-1, the eigenvector is reflected.\\
\end{itemize}



\textbf{QR ALGORITHM:}\\
The QR Algorithm is particularly useful for dense,square matrices. In this method the matrix $A$ is factorized into Q and R matrices where Q is an orthogonal matrix and R is upper triangular matrix.\\
\textit{Step 1:} $A_0$=$A$\\
\textit{Step 2:} $A_k$=$Q_k R_k$\\
\textit{Step 3:} $A_{k+1}$=$R_k Q_k$ where $A_{k+1}$ is an upper triangular matrix whose diagonal elements are the eigen values of $A$.\\

\textbf{Example 1:}\\
Consider the matrix, 
\[
	A= \begin{bmatrix}
		6 & 2\\
		2 & 5
	\end{bmatrix}
\]

The QR decomposition gives us:
\[
Q_0 = \begin{bmatrix}
	\frac{3}{\sqrt{10}} & \frac{-13}{\sqrt{10}} \\
	\frac{1}{\sqrt{1690}} & \frac{39}{\sqrt{1690}}
\end{bmatrix}
\quad \text{and} \quad
R_0 = \begin{bmatrix}
	\frac{20}{\sqrt{10}} & \frac{11}{\sqrt{10}} \\
\end{bmatrix}
\]

Updated matrix \(A_1\)
\[
	A_1=RQ=\begin{bmatrix}
		7 & 0\\
		0 & 4
	\end{bmatrix}
\]

Thus, the eigenvalues are 7 and 4.\\
\textbf{Python code:Software Assignment/codes/e1.py.}\\
\begin{lstlisting}[language=Python, caption=QR Algorithm Python]
import numpy as np

def qr_decomposition(A):
    """Compute the QR decomposition of matrix A using Gram-Schmidt process"""
    n = A.shape[0]
    Q = np.zeros((n, n))
    R = np.zeros((n, n))
    
    for k in range(n):
        # v is the k-th column of A
        v = A[:, k]
        
        for i in range(k):
            # Project onto previous q's
            R[i, k] = np.dot(Q[:, i], v)
            v = v - R[i, k] * Q[:, i]
        
        # Normalize the vector to get q
        R[k, k] = np.linalg.norm(v)
        Q[:, k] = v / R[k, k]
    
    return Q, R

def qr_algorithm(A, iterations=100):
    """Perform the QR algorithm to compute eigenvalues of A"""
    A_copy = A.copy()
    n = A_copy.shape[0]
    
    for _ in range(iterations):
        # Step 1: QR decomposition of A
        Q, R = qr_decomposition(A_copy)
      
        # Step 2: A = R * Q (new A)
        A_copy = np.dot(R, Q)
    
    # Return the diagonal elements as eigenvalues
    return np.diag(A_copy)

# Matrix A = [[6, 2], [2, 5]]
A = np.array([[6, 2],
              [2, 5]])

# Run QR algorithm
eigenvalues = qr_algorithm(A, iterations=100)

print("Eigenvalues of the matrix A:")
print(eigenvalues)
\end{lstlisting}

\textbf{Example 2:}\\
Consider the matrix \( A \),

\[
A = \begin{bmatrix}
	4 & 1 \\
	2 & 3
\end{bmatrix}
\]

The QR decomposition gives us:

\[
Q_0 = \begin{bmatrix}
	\frac{2}{\sqrt{5}} & \frac{1}{\sqrt{5}} \\
	\frac{1}{\sqrt{5}} & \frac{3}{\sqrt{5}}
\end{bmatrix}
\quad \text{and} \quad
R_0 = \begin{bmatrix}
	\sqrt{5} & \frac{11}{\sqrt{5}} \\
	0 & \sqrt{5}
\end{bmatrix}
\]

The updated matrix \( A_1 \) after the first QR iteration is:

\[
A_1 = \begin{bmatrix}
	4.618 & 1.382 \\
	1.382 & 3.618
\end{bmatrix}
\]

After further iterations, the matrix converges to:

\[
A_k = \begin{bmatrix}
	5 & 0 \\
	0 & 2
\end{bmatrix}
\]

Thus, the eigenvalues of the matrix \( A \) are \( 5 \) and \( 2 \).\\
\textbf{Python code:Software Assignment/codes/e2.py.}\\
\begin{lstlisting}
import numpy as np

def qr_decomposition(A):
    """
    Perform the QR decomposition using Gram-Schmidt process.
    Given a matrix A, return orthogonal matrix Q and upper triangular matrix R such that A = Q * R.
    """
    n = A.shape[0]
    Q = np.zeros((n, n))
    R = np.zeros((n, n))
    
    for k in range(n):
        # v is the k-th column of A
        v = A[:, k]
        
        # Subtract projections onto previous q's
        for i in range(k):
            R[i, k] = np.dot(Q[:, i], v)
            v = v - R[i, k] * Q[:, i]
        
        # Normalize the vector to get q
        R[k, k] = np.linalg.norm(v)
        Q[:, k] = v / R[k, k]
    
    return Q, R

def qr_algorithm(A, iterations=100):
    """
    Perform the QR algorithm to compute the eigenvalues of A.
    Repeat the QR decomposition process and update A = R * Q for a given number of iterations.
    """
    A_copy = A.copy()
    for _ in range(iterations):
        Q, R = qr_decomposition(A_copy)
        A_copy = np.dot(R, Q)
    
    # Return the diagonal elements as the eigenvalues
    return np.diag(A_copy)

# Given matrix A = [[6, 2], [2, 5]]
A = np.array([[4, 1],
              [2, 3]])

# Run QR algorithm
eigenvalues = qr_algorithm(A, iterations=100)

# Output the eigenvalues
print("Eigenvalues of the matrix A:")
print(eigenvalues)
\end{lstlisting}

\textbf{Example 3:}\\
Cosider the $3 \times 3$ matrix
\[
	A=\begin{bmatrix}
		4 & -1 & 1\\
		-1 & 4 & -2\\
		1 & -2 & 3\\
	\end{bmatrix}
\]
\[
Q_0 = \begin{bmatrix}
	\frac{4}{\sqrt{18}} & \frac{11}{\sqrt{1251}} & \frac{-2}{\sqrt{6}} \\
	\frac{-1}{\sqrt{18}} & \frac{31}{\sqrt{1251}} & \frac{1}{\sqrt{6}}\\
	\frac{1}{\sqrt{18}} & \frac{-13}{\sqrt{1251}} & \frac{1}{\sqrt{6}}
\end{bmatrix}
\quad \text{and} \quad
R_0 = \begin{bmatrix}
	\sqrt{18} & 0 & \frac{-8}{\sqrt{6}} \\
	0 & \sqrt{15} & \frac{4}{\sqrt{6}}\\
	0 & 0 & \frac{-1}{\sqrt{6}}\\
\end{bmatrix}
\]

After repetitively applying QR algorithm,\\
\[
	A_k=\begin{bmatrix}
		6 & 0 & 0\\
		0 & 3 & 0\\
		0 & 0 & 2\\
	\end{bmatrix}
\]

Thus, the eigenvalues are 6, 3 and 2.\\
\textbf{Python code:Software Assignment/codes/e3.py.}\\
\begin{lstlisting}
import numpy as np

def qr_decomposition(A):
    """
    Perform the QR decomposition using Gram-Schmidt process.
    Given a matrix A, return orthogonal matrix Q and upper triangular matrix R such that A = Q * R.
    """
    n = A.shape[0]
    Q = np.zeros((n, n))
    R = np.zeros((n, n))
    
    for k in range(n):
        v = A[:, k]
        
        # Subtract projections onto previous q's
        for i in range(k):
            R[i, k] = np.dot(Q[:, i], v)
            v = v - R[i, k] * Q[:, i]
        
        # Normalize the vector to get q
        R[k, k] = np.linalg.norm(v)
        Q[:, k] = v / R[k, k]
    
    return Q, R

def qr_algorithm(A, iterations=100):
    """
    Perform the QR algorithm to compute the eigenvalues of A.
    Repeat the QR decomposition process and update A = R * Q for a given number of iterations.
    """
    A_copy = A.copy()
    for _ in range(iterations):
        Q, R = qr_decomposition(A_copy)
        A_copy = np.dot(R, Q)
    
    # Return the diagonal elements as the eigenvalues
    return np.diag(A_copy)

# Given matrix A
A = np.array([[4, -1, 1],
              [-1, 4, -2],
              [1, -2, 3]])

# Run QR algorithm
eigenvalues = qr_algorithm(A, iterations=100)

# Output the eigenvalues
print("Eigenvalues of the matrix A:")
print(eigenvalues)
\end{lstlisting}

\textbf{Few of the other methods to find eigenvalues are:}
\begin{itemize}
	\item Power Iteration Algorithm
	\item Jacobi Method
	\item Davidson Method\\
\end{itemize}

\begin{table}[h!]
\centering
\begin{tabular}[13pt]{ |c|c|c|c|c|}
	\hline
	\textbf{Algorithm} & \textbf{Time Complexity} & \textbf{Convergence Rate} & \textbf{Suitable for}\\
	\hline
	Power Iteration & \(O(n^2)\) per iteration & Slow(converges linearly) & Large Matrices, dominant eigenvalues \\
	\hline
	QR Algorithm & \(O(n^3)\) & Fast(superlinear or cubic) & General,dense,Symmetric matrices \\
	\hline
	Jacobi Method & \(O(n^3)\) & Moderate(linear and sublinear) & Symmetric Matrices \\
	\hline
	Davidson Method & \(O(mn^2)\) per iteration & Fast(superlinear or cubic) & Large sparse Matrices, few eigenvalues \\
	\hline
\end{tabular}
\caption{Some properties of the algorithms}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{|l|p{7cm}|p{7cm}|}
	\hline
	\textbf{Algorithm}          & \textbf{Pros}                                                 & \textbf{Cons}                                                 \\
	\hline
	Power Iteration             & Simple to implement, fast for dominant eigenvalue            & Slow for finding all eigenvalues, requires good starting vector \\ 
	\hline
	QR Algorithm                & Finds all eigenvalues, converges relatively quickly           & High computational cost per iteration for large matrices       \\ 
	\hline
	Jacobi Method               & Highly accurate for symmetric matrices, easy to implement     & Slower for large matrices, less efficient for non-symmetric     \\ 
	\hline
	Davidson's Method           & Efficient for large sparse matrices, computes few eigenvalues & Requires preconditioning, sensitive to poor initial guess     \\ 
	\hline
\end{tabular}
\caption{Pros and Cons of Various Eigenvalue Algorithms}
\end{table}

\end{document}
